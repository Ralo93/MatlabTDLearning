## Table of contents
* [General info](#general-info)
* [Requirements](#requirements)
* [Technologies](#technologies)
* [Illustrations](#illustrations)
* [Project status](#project-status)


## General info
A Temporal Difference (TD) Learning approach (SARSA in this case) for a modelled mikroactuator, shaped like a hemisphere.
Also, a parallel Q-Learning approach is tested and compared. <br>
Right now I translate the dynamics into the 3D-version, experiment with the expsilon parameter for more exploration and also experiment with positive instead of negative rewards.

## Sources
In collaboration with University of Augsburg. <br>
good source: https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce

## Requirements
Octave1.0

## Technologies
Octave1.0

## Illustrations
<img src="https://user-images.githubusercontent.com/78420756/109026600-33376980-76c0-11eb-9154-674b188818f3.png" width="260" height="200"> <img src="https://user-images.githubusercontent.com/78420756/109413524-31b2bd80-79ae-11eb-8086-ac63b7592757.png" width="280" height="200"> <br>
Source: Michael Olbrich Universit√§t Augsburg

<img src="https://user-images.githubusercontent.com/78420756/109413719-18f6d780-79af-11eb-9b0b-b8d82debbd10.PNG" width="800" height="200"> <br>
Left: A sinus trajectory followed by our TD Agent. Mid: The (negative) rewards collected. Right: The exploration parameter epsilon.
  
## Project status
Active
